{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2d1c7a8-fc93-46d1-9d31-df819bf02034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_from_disk\n",
    "import numpy as np\n",
    "\n",
    "# Paths\n",
    "processed_dir = \"/home/aac/project-hyperion/data/processed\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5de9fe3-c380-45a9-b9e4-d3318b16c0d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models loaded.\n"
     ]
    }
   ],
   "source": [
    "# Load tokenized WikiText-2\n",
    "wikitext2 = load_from_disk(os.path.join(processed_dir, \"wikitext2_tokenized\"))\n",
    "class WikiText2TorchDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, hf_dataset, split=\"train\"):\n",
    "        self.data = hf_dataset[split]\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        input_ids = torch.tensor(item[\"input_ids\"], dtype=torch.long)\n",
    "        attention_mask = torch.tensor(item[\"attention_mask\"], dtype=torch.long)\n",
    "        return input_ids, attention_mask\n",
    "\n",
    "wikitext2_train_ds = WikiText2TorchDataset(wikitext2, split=\"train\")\n",
    "wikitext2_loader = DataLoader(wikitext2_train_ds, batch_size=32, shuffle=True, num_workers=2)\n",
    "\n",
    "# Load CIFAR-10\n",
    "cifar10_train = torch.load(os.path.join(processed_dir, \"cifar10_train.pt\"))\n",
    "class CIFAR10TorchDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        img, label = self.data[idx]\n",
    "        return img, label\n",
    "\n",
    "cifar10_train_ds = CIFAR10TorchDataset(cifar10_train)\n",
    "cifar10_loader = DataLoader(cifar10_train_ds, batch_size=32, shuffle=True, num_workers=2)\n",
    "\n",
    "# Load models\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "class SimpleTransformerLM(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=256, n_heads=4, n_layers=2, max_len=128):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=emb_dim, nhead=n_heads)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        self.fc = nn.Linear(emb_dim, vocab_size)\n",
    "        self.max_len = max_len\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        x = self.embedding(input_ids)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        x = self.transformer(x)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        logits = self.fc(x)\n",
    "        return logits\n",
    "\n",
    "model_lm = SimpleTransformerLM(vocab_size).to(device)\n",
    "model_lm.load_state_dict(torch.load(os.path.join(processed_dir, \"simple_transformer_lm.pt\"), map_location=device))\n",
    "\n",
    "model_cifar = models.resnet18(num_classes=10).to(device)\n",
    "model_cifar.load_state_dict(torch.load(os.path.join(processed_dir, \"resnet18_cifar10.pt\"), map_location=device))\n",
    "\n",
    "print(\"Models loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f8c9ed7-e0b6-4ddc-8430-275bee1e0c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline memory usage: Language model\n",
      "Before forward pass: CUDA memory allocated: 152.93 MB, reserved: 230.00 MB\n",
      "After forward pass: CUDA memory allocated: 932.96 MB, reserved: 1042.00 MB\n",
      "\n",
      "Baseline memory usage: CIFAR-10 model\n",
      "Before forward pass: CUDA memory allocated: 932.96 MB, reserved: 1042.00 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/functional.py:5504: UserWarning: 1Torch was not compiled with memory efficient attention. (Triggered internally at /var/lib/jenkins/pytorch/aten/src/ATen/native/transformers/hip/sdp_utils.cpp:505.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After forward pass: CUDA memory allocated: 153.34 MB, reserved: 958.00 MB\n"
     ]
    }
   ],
   "source": [
    "def print_memory(prefix=\"\"):\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**2\n",
    "        reserved = torch.cuda.memory_reserved() / 1024**2\n",
    "        print(f\"{prefix} CUDA memory allocated: {allocated:.2f} MB, reserved: {reserved:.2f} MB\")\n",
    "    else:\n",
    "        print(f\"{prefix} CUDA not available; memory stats not shown.\")\n",
    "\n",
    "# Baseline for language model\n",
    "print(\"Baseline memory usage: Language model\")\n",
    "print_memory(\"Before forward pass:\")\n",
    "input_ids, attention_mask = next(iter(wikitext2_loader))\n",
    "input_ids = input_ids.to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = model_lm(input_ids[:, :-1])\n",
    "print_memory(\"After forward pass:\")\n",
    "\n",
    "# Baseline for vision model\n",
    "print(\"\\nBaseline memory usage: CIFAR-10 model\")\n",
    "print_memory(\"Before forward pass:\")\n",
    "images, labels = next(iter(cifar10_loader))\n",
    "images = images.to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = model_cifar(images)\n",
    "print_memory(\"After forward pass:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cc85f42-abf3-45df-a053-e680ce79cca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpointed language model ready.\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.checkpoint import checkpoint_sequential\n",
    "\n",
    "class SimpleTransformerLM(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=256, n_heads=4, n_layers=2, max_len=128, use_checkpoint=False):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=emb_dim, nhead=n_heads)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        self.fc = nn.Linear(emb_dim, vocab_size)\n",
    "        self.max_len = max_len\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        x = self.embedding(input_ids)\n",
    "        x = x.permute(1, 0, 2)  # (seq, batch, emb)\n",
    "        if self.use_checkpoint:\n",
    "            # Use checkpointing on transformer layers\n",
    "            x = checkpoint_sequential(self.transformer.layers, len(self.transformer.layers), x)\n",
    "        else:\n",
    "            x = self.transformer(x)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        logits = self.fc(x)\n",
    "        return logits\n",
    "\n",
    "# Instantiate with checkpointing enabled\n",
    "model_lm_ckpt = SimpleTransformerLM(\n",
    "    vocab_size,\n",
    "    emb_dim=256,\n",
    "    n_heads=4,\n",
    "    n_layers=2,\n",
    "    max_len=128,\n",
    "    use_checkpoint=True\n",
    ").to(device)\n",
    "model_lm_ckpt.load_state_dict(model_lm.state_dict())\n",
    "print(\"Checkpointed language model ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94a6e927-6097-435d-a400-e35b979a40c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpointed ResNet-18 ready.\n"
     ]
    }
   ],
   "source": [
    "def checkpoint_resnet_blocks(model):\n",
    "    def forward_with_ckpt(x):\n",
    "        x = model.conv1(x)\n",
    "        x = model.bn1(x)\n",
    "        x = model.relu(x)\n",
    "        x = model.maxpool(x)\n",
    "        layers = [model.layer1, model.layer2, model.layer3, model.layer4]\n",
    "        x = checkpoint_sequential(layers, len(layers), x)\n",
    "        x = model.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = model.fc(x)\n",
    "        return x\n",
    "    model.forward = forward_with_ckpt\n",
    "    return model\n",
    "\n",
    "# Apply checkpointing to your already-loaded model_cifar\n",
    "model_cifar_ckpt = checkpoint_resnet_blocks(model_cifar)\n",
    "print(\"Checkpointed ResNet-18 ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68d1929a-b5e5-4a1f-aa17-07fcb448ca35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: Language model with activation checkpointing\n",
      "Before forward pass: CUDA memory allocated: 370.23 MB, reserved: 962.00 MB\n",
      "After forward pass: CUDA memory allocated: 1150.23 MB, reserved: 1742.00 MB\n",
      "\n",
      "Memory usage: ResNet-18 with activation checkpointing\n",
      "Before forward pass: CUDA memory allocated: 1150.23 MB, reserved: 1742.00 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/utils/checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/utils/checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After forward pass: CUDA memory allocated: 370.23 MB, reserved: 1744.00 MB\n"
     ]
    }
   ],
   "source": [
    "# For language model with checkpointing\n",
    "print(\"Memory usage: Language model with activation checkpointing\")\n",
    "print_memory(\"Before forward pass:\")\n",
    "input_ids, attention_mask = next(iter(wikitext2_loader))\n",
    "input_ids = input_ids.to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = model_lm_ckpt(input_ids[:, :-1])\n",
    "print_memory(\"After forward pass:\")\n",
    "\n",
    "# For ResNet-18 with checkpointing\n",
    "print(\"\\nMemory usage: ResNet-18 with activation checkpointing\")\n",
    "print_memory(\"Before forward pass:\")\n",
    "images, labels = next(iter(cifar10_loader))\n",
    "images = images.to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = model_cifar_ckpt(images)\n",
    "print_memory(\"After forward pass:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b241940-ed11-45b4-8b56-91614448cade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training one batch (no checkpointing):\n",
      "Before: CUDA memory allocated: 370.23 MB, reserved: 962.00 MB\n",
      "After: CUDA memory allocated: 478.68 MB, reserved: 4090.00 MB\n",
      "Loss: 2.8528\n",
      "\n",
      "Training one batch (with checkpointing):\n",
      "Before: CUDA memory allocated: 478.68 MB, reserved: 964.00 MB\n",
      "After: CUDA memory allocated: 587.06 MB, reserved: 4092.00 MB\n",
      "Loss: 2.7724\n"
     ]
    }
   ],
   "source": [
    "def train_one_batch(model, dataloader, device):\n",
    "    model.train()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4)\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "    input_ids, attention_mask = next(iter(dataloader))\n",
    "    input_ids = input_ids.to(device)\n",
    "    targets = input_ids[:, 1:].contiguous()\n",
    "    inputs = input_ids[:, :-1].contiguous()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(inputs)\n",
    "    logits = outputs.reshape(-1, outputs.size(-1))\n",
    "    targets = targets.reshape(-1)\n",
    "    loss = loss_fn(logits, targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "# Clear memory\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Training one batch (no checkpointing):\")\n",
    "print_memory(\"Before:\")\n",
    "loss = train_one_batch(model_lm, wikitext2_loader, device)\n",
    "print_memory(\"After:\")\n",
    "print(f\"Loss: {loss:.4f}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\nTraining one batch (with checkpointing):\")\n",
    "print_memory(\"Before:\")\n",
    "loss = train_one_batch(model_lm_ckpt, wikitext2_loader, device)\n",
    "print_memory(\"After:\")\n",
    "print(f\"Loss: {loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
