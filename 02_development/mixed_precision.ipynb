{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e6f2bfc-9350-4615-9ffd-14afbe3e8da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_from_disk\n",
    "import numpy as np\n",
    "\n",
    "# Paths\n",
    "processed_dir = \"/home/aac/project-hyperion/data/processed\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2eea1c06-cc88-48ae-b6d9-ea53d2a26588",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models initialized.\n"
     ]
    }
   ],
   "source": [
    "# Load WikiText-2\n",
    "wikitext2 = load_from_disk(os.path.join(processed_dir, \"wikitext2_tokenized\"))\n",
    "class WikiText2TorchDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, hf_dataset, split=\"train\"):\n",
    "        self.data = hf_dataset[split]\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        input_ids = torch.tensor(item[\"input_ids\"], dtype=torch.long)\n",
    "        attention_mask = torch.tensor(item[\"attention_mask\"], dtype=torch.long)\n",
    "        return input_ids, attention_mask\n",
    "\n",
    "wikitext2_train_ds = WikiText2TorchDataset(wikitext2, split=\"train\")\n",
    "wikitext2_loader = DataLoader(wikitext2_train_ds, batch_size=32, shuffle=True, num_workers=2)\n",
    "\n",
    "# Load CIFAR-10\n",
    "cifar10_train = torch.load(os.path.join(processed_dir, \"cifar10_train.pt\"))\n",
    "class CIFAR10TorchDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        img, label = self.data[idx]\n",
    "        return img, label\n",
    "\n",
    "cifar10_train_ds = CIFAR10TorchDataset(cifar10_train)\n",
    "cifar10_loader = DataLoader(cifar10_train_ds, batch_size=32, shuffle=True, num_workers=2)\n",
    "\n",
    "# Load tokenizer and get vocab size\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "# Define language model\n",
    "class SimpleTransformerLM(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=256, n_heads=4, n_layers=2, max_len=128):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=emb_dim, nhead=n_heads)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        self.fc = nn.Linear(emb_dim, vocab_size)\n",
    "        self.max_len = max_len\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        x = self.embedding(input_ids)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        x = self.transformer(x)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        logits = self.fc(x)\n",
    "        return logits\n",
    "\n",
    "model_lm = SimpleTransformerLM(vocab_size).to(device)\n",
    "model_cifar = models.resnet18(num_classes=10).to(device)\n",
    "print(\"Models initialized.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1ab001a-b3c1-4fa3-ad01-248a1d6a42cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "def train_language_model_amp(model, dataloader, device, epochs=3):\n",
    "    model.train()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4)\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "    scaler = GradScaler()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for input_ids, attention_mask in dataloader:\n",
    "            input_ids = input_ids.to(device)\n",
    "            targets = input_ids[:, 1:].contiguous()\n",
    "            inputs = input_ids[:, :-1].contiguous()\n",
    "            optimizer.zero_grad()\n",
    "            with autocast():  # No device_type argument for ROCm/CUDA\n",
    "                outputs = model(inputs)\n",
    "                logits = outputs.reshape(-1, outputs.size(-1))\n",
    "                targets_flat = targets.reshape(-1)\n",
    "                loss = loss_fn(logits, targets_flat)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch+1}: Avg Loss = {avg_loss:.4f}\")\n",
    "\n",
    "def train_cifar_model_amp(model, dataloader, device, epochs=3):\n",
    "    model.train()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    scaler = GradScaler()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        total_correct = 0\n",
    "        total = 0\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            with autocast():\n",
    "                outputs = model(images)\n",
    "                loss = loss_fn(outputs, labels)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            total_loss += loss.item()\n",
    "            _, preds = outputs.max(1)\n",
    "            total_correct += preds.eq(labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        acc = 100 * total_correct / total\n",
    "        print(f\"Epoch {epoch+1}: Loss = {avg_loss:.4f}, Acc = {acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "247df270-650d-42d1-8b06-12cc2cf4b275",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9995387c-2420-49c9-8067-c0cf5709003e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training language model on WikiText-2 with AMP (25 epochs)...\n",
      "Epoch 1: Avg Loss = 5.1700\n",
      "Epoch 2: Avg Loss = 4.9256\n",
      "Epoch 3: Avg Loss = 4.7180\n",
      "Epoch 4: Avg Loss = 4.5350\n",
      "Epoch 5: Avg Loss = 4.3713\n",
      "Epoch 6: Avg Loss = 4.2223\n",
      "Epoch 7: Avg Loss = 4.0838\n",
      "Epoch 8: Avg Loss = 3.9572\n",
      "Epoch 9: Avg Loss = 3.8424\n",
      "Epoch 10: Avg Loss = 3.7347\n",
      "Epoch 11: Avg Loss = 3.6362\n",
      "Epoch 12: Avg Loss = 3.5438\n",
      "Epoch 13: Avg Loss = 3.4593\n",
      "Epoch 14: Avg Loss = 3.3817\n",
      "Epoch 15: Avg Loss = 3.3098\n",
      "Epoch 16: Avg Loss = 3.2431\n",
      "Epoch 17: Avg Loss = 3.1796\n",
      "Epoch 18: Avg Loss = 3.1202\n",
      "Epoch 19: Avg Loss = 3.0667\n",
      "Epoch 20: Avg Loss = 3.0152\n",
      "Epoch 21: Avg Loss = 2.9660\n",
      "Epoch 22: Avg Loss = 2.9223\n",
      "Epoch 23: Avg Loss = 2.8776\n",
      "Epoch 24: Avg Loss = 2.8380\n",
      "Epoch 25: Avg Loss = 2.7995\n",
      "Training ResNet-18 on CIFAR-10 with AMP (25 epochs)...\n",
      "Epoch 1: Loss = 0.7140, Acc = 75.54%\n",
      "Epoch 2: Loss = 0.6040, Acc = 79.14%\n",
      "Epoch 3: Loss = 0.5048, Acc = 82.61%\n",
      "Epoch 4: Loss = 0.4257, Acc = 85.37%\n",
      "Epoch 5: Loss = 0.3431, Acc = 88.11%\n",
      "Epoch 6: Loss = 0.2831, Acc = 90.14%\n",
      "Epoch 7: Loss = 0.2307, Acc = 92.02%\n",
      "Epoch 8: Loss = 0.1914, Acc = 93.28%\n",
      "Epoch 9: Loss = 0.1609, Acc = 94.30%\n",
      "Epoch 10: Loss = 0.1510, Acc = 94.74%\n",
      "Epoch 11: Loss = 0.1295, Acc = 95.51%\n",
      "Epoch 12: Loss = 0.1186, Acc = 95.86%\n",
      "Epoch 13: Loss = 0.1075, Acc = 96.33%\n",
      "Epoch 14: Loss = 0.1058, Acc = 96.39%\n",
      "Epoch 15: Loss = 0.0956, Acc = 96.71%\n",
      "Epoch 16: Loss = 0.0898, Acc = 96.92%\n",
      "Epoch 17: Loss = 0.0881, Acc = 96.98%\n",
      "Epoch 18: Loss = 0.0802, Acc = 97.22%\n",
      "Epoch 19: Loss = 0.0802, Acc = 97.25%\n",
      "Epoch 20: Loss = 0.0748, Acc = 97.47%\n",
      "Epoch 21: Loss = 0.0764, Acc = 97.38%\n",
      "Epoch 22: Loss = 0.0774, Acc = 97.33%\n",
      "Epoch 23: Loss = 0.0669, Acc = 97.74%\n",
      "Epoch 24: Loss = 0.0671, Acc = 97.65%\n",
      "Epoch 25: Loss = 0.0642, Acc = 97.76%\n"
     ]
    }
   ],
   "source": [
    "# Train language model with AMP\n",
    "print(\"Training language model on WikiText-2 with AMP (25 epochs)...\")\n",
    "train_language_model_amp(model_lm, wikitext2_loader, device, epochs=25)\n",
    "\n",
    "# Train ResNet-18 with AMP\n",
    "print(\"Training ResNet-18 on CIFAR-10 with AMP (25 epochs)...\")\n",
    "train_cifar_model_amp(model_cifar, cifar10_loader, device, epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9bf73118-f63c-4db4-a3ec-944522784469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AMP-trained models and scaler state saved.\n"
     ]
    }
   ],
   "source": [
    "# Save AMP-trained models and scaler state for reproducibility\n",
    "from torch.cuda.amp import GradScaler\n",
    "\n",
    "scaler_lm = GradScaler()\n",
    "scaler_cifar = GradScaler()\n",
    "\n",
    "torch.save({\n",
    "    \"model_state_dict\": model_lm.state_dict(),\n",
    "    \"scaler_state_dict\": scaler_lm.state_dict()\n",
    "}, os.path.join(processed_dir, \"simple_transformer_lm_amp.pt\"))\n",
    "\n",
    "torch.save({\n",
    "    \"model_state_dict\": model_cifar.state_dict(),\n",
    "    \"scaler_state_dict\": scaler_cifar.state_dict()\n",
    "}, os.path.join(processed_dir, \"resnet18_cifar10_amp.pt\"))\n",
    "\n",
    "print(\"AMP-trained models and scaler state saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7840f34c-2c1d-4891-81c5-a6f6a6444e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profiling AMP training (language model, 10 batches)...\n",
      "AMP training: 10 batches in 0.64 sec\n",
      "Peak memory used: 3465.58 MB\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "def profile_amp_training(model, dataloader, device, n_batches=10):\n",
    "    model.eval()\n",
    "    scaler = GradScaler()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4)\n",
    "    start = time.time()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    for i, (input_ids, attention_mask) in enumerate(dataloader):\n",
    "        if i >= n_batches:\n",
    "            break\n",
    "        input_ids = input_ids.to(device)\n",
    "        targets = input_ids[:, 1:].contiguous()\n",
    "        inputs = input_ids[:, :-1].contiguous()\n",
    "        optimizer.zero_grad()\n",
    "        with autocast():  # <-- FIXED: no device_type argument\n",
    "            outputs = model(inputs)\n",
    "            logits = outputs.reshape(-1, outputs.size(-1))\n",
    "            targets_flat = targets.reshape(-1)\n",
    "            loss = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)(logits, targets_flat)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "    torch.cuda.synchronize()\n",
    "    end = time.time()\n",
    "    print(f\"AMP training: {n_batches} batches in {end-start:.2f} sec\")\n",
    "    print(f\"Peak memory used: {torch.cuda.max_memory_allocated() / 1024**2:.2f} MB\")\n",
    "\n",
    "print(\"Profiling AMP training (language model, 10 batches)...\")\n",
    "profile_amp_training(model_lm, wikitext2_loader, device, n_batches=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
