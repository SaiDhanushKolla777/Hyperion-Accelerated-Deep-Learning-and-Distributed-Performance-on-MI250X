{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c198d582-468e-4de7-9cc2-47fc89585596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 2.3.0a0+git96dd291\n",
      "numpy: 1.24.4\n",
      "CUDA/ROCm device count: 1\n",
      "Current device: 0\n",
      "Device name: AMD Instinct MI250X/MI250\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Check versions for compatibility\n",
    "print(\"torch:\", torch.__version__)\n",
    "print(\"numpy:\", np.__version__)\n",
    "\n",
    "# Check for AMD GPU (ROCm)\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA/ROCm device count:\", torch.cuda.device_count())\n",
    "    print(\"Current device:\", torch.cuda.current_device())\n",
    "    print(\"Device name:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"No CUDA/ROCm device detected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d62b075-86ed-47af-87a9-ccfc5b19d21b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tokenized WikiText-2 splits: DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask'],\n",
      "        num_rows: 2891\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask'],\n",
      "        num_rows: 23767\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'attention_mask'],\n",
      "        num_rows: 2461\n",
      "    })\n",
      "})\n",
      "Sample input_ids: [796, 569, 18354, 7496, 17740, 6711, 796, 220, 198, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
      "Sample attention_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "processed_dir = \"/home/aac/project-hyperion/data/processed\"\n",
    "wikitext2_path = os.path.join(processed_dir, \"wikitext2_tokenized\")\n",
    "wikitext2 = load_from_disk(wikitext2_path)\n",
    "\n",
    "print(\"Loaded tokenized WikiText-2 splits:\", wikitext2)\n",
    "print(\"Sample input_ids:\", wikitext2[\"train\"][0][\"input_ids\"][:20])\n",
    "print(\"Sample attention_mask:\", wikitext2[\"train\"][0][\"attention_mask\"][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "662c9d40-b3a9-4a61-b93e-7335cd863ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIFAR-10 train samples: 50000\n",
      "CIFAR-10 test samples: 10000\n",
      "Sample image shape: torch.Size([3, 32, 32]) Label: 6\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "cifar10_train = torch.load(os.path.join(processed_dir, \"cifar10_train.pt\"))\n",
    "cifar10_test = torch.load(os.path.join(processed_dir, \"cifar10_test.pt\"))\n",
    "\n",
    "print(\"CIFAR-10 train samples:\", len(cifar10_train))\n",
    "print(\"CIFAR-10 test samples:\", len(cifar10_test))\n",
    "img, lbl = cifar10_train[0]\n",
    "print(\"Sample image shape:\", img.shape, \"Label:\", lbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc886b8d-6a54-493e-a56f-b2676286db0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoaders ready.\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# For WikiText-2, wrap Hugging Face dataset in PyTorch Dataset for DataLoader\n",
    "class WikiText2TorchDataset(Dataset):\n",
    "    def __init__(self, hf_dataset, split=\"train\"):\n",
    "        self.data = hf_dataset[split]\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        input_ids = torch.tensor(item[\"input_ids\"], dtype=torch.long)\n",
    "        attention_mask = torch.tensor(item[\"attention_mask\"], dtype=torch.long)\n",
    "        return input_ids, attention_mask\n",
    "\n",
    "# For CIFAR-10, data is already a list of (img, label) tuples\n",
    "class CIFAR10TorchDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        img, label = self.data[idx]\n",
    "        return img, label\n",
    "\n",
    "# Instantiate datasets\n",
    "wikitext2_train_ds = WikiText2TorchDataset(wikitext2, split=\"train\")\n",
    "cifar10_train_ds = CIFAR10TorchDataset(cifar10_train)\n",
    "\n",
    "# DataLoaders\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "wikitext2_loader = DataLoader(wikitext2_train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "cifar10_loader = DataLoader(cifar10_train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "\n",
    "print(\"DataLoaders ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1665426-40f3-4f46-a225-9b41e37bffa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleTransformerLM initialized.\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SimpleTransformerLM(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=256, n_heads=4, n_layers=2, max_len=128):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=emb_dim, nhead=n_heads)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        self.fc = nn.Linear(emb_dim, vocab_size)\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        x = self.embedding(input_ids)  # (batch, seq, emb)\n",
    "        x = x.permute(1, 0, 2)        # (seq, batch, emb)\n",
    "        x = self.transformer(x)        # (seq, batch, emb)\n",
    "        x = x.permute(1, 0, 2)        # (batch, seq, emb)\n",
    "        logits = self.fc(x)            # (batch, seq, vocab)\n",
    "        return logits\n",
    "\n",
    "# Get vocab size from tokenizer config\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "model_lm = SimpleTransformerLM(vocab_size).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"SimpleTransformerLM initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0df8353-1bd1-4819-b98e-c71d2c61636c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet-18 for CIFAR-10 initialized.\n"
     ]
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "# Use a small ResNet for CIFAR-10 (ResNet-18)\n",
    "model_cifar = models.resnet18(num_classes=10)\n",
    "model_cifar = model_cifar.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"ResNet-18 for CIFAR-10 initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1aa6f14f-6e8e-4b9e-a8c2-e0102528061a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_language_model(model, dataloader, device, epochs=1):\n",
    "    model.train()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4)\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for input_ids, attention_mask in dataloader:\n",
    "            input_ids = input_ids.to(device)\n",
    "            # Shift targets for language modeling\n",
    "            targets = input_ids[:, 1:].contiguous()\n",
    "            inputs = input_ids[:, :-1].contiguous()\n",
    "            outputs = model(inputs)\n",
    "            logits = outputs[:, :, :]\n",
    "            logits = logits.reshape(-1, logits.size(-1))\n",
    "            targets = targets.reshape(-1)\n",
    "            loss = loss_fn(logits, targets)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}: Avg Loss = {total_loss / len(dataloader):.4f}\")\n",
    "\n",
    "def train_cifar_model(model, dataloader, device, epochs=1):\n",
    "    model.train()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        total_correct = 0\n",
    "        total = 0\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            _, preds = outputs.max(1)\n",
    "            total_correct += preds.eq(labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "        print(f\"Epoch {epoch+1}: Loss = {total_loss / len(dataloader):.4f}, Acc = {100 * total_correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05f3620a-01eb-4c54-97ad-e06bbe833403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training language model on WikiText-2 (25 epoch, sanity check)...\n",
      "Epoch 1: Avg Loss = 5.6753\n",
      "Epoch 2: Avg Loss = 5.3459\n",
      "Epoch 3: Avg Loss = 5.0855\n",
      "Epoch 4: Avg Loss = 4.8599\n",
      "Epoch 5: Avg Loss = 4.6623\n",
      "Epoch 6: Avg Loss = 4.4856\n",
      "Epoch 7: Avg Loss = 4.3231\n",
      "Epoch 8: Avg Loss = 4.1774\n",
      "Epoch 9: Avg Loss = 4.0440\n",
      "Epoch 10: Avg Loss = 3.9218\n",
      "Epoch 11: Avg Loss = 3.8100\n",
      "Epoch 12: Avg Loss = 3.7063\n",
      "Epoch 13: Avg Loss = 3.6117\n",
      "Epoch 14: Avg Loss = 3.5219\n",
      "Epoch 15: Avg Loss = 3.4402\n",
      "Epoch 16: Avg Loss = 3.3657\n",
      "Epoch 17: Avg Loss = 3.2932\n",
      "Epoch 18: Avg Loss = 3.2273\n",
      "Epoch 19: Avg Loss = 3.1675\n",
      "Epoch 20: Avg Loss = 3.1089\n",
      "Epoch 21: Avg Loss = 3.0561\n",
      "Epoch 22: Avg Loss = 3.0041\n",
      "Epoch 23: Avg Loss = 2.9584\n",
      "Epoch 24: Avg Loss = 2.9118\n",
      "Epoch 25: Avg Loss = 2.8712\n",
      "Training ResNet-18 on CIFAR-10 (25 epoch, sanity check)...\n",
      "Epoch 1: Loss = 1.0457, Acc = 63.47%\n",
      "Epoch 2: Loss = 0.8336, Acc = 71.07%\n",
      "Epoch 3: Loss = 0.6990, Acc = 75.95%\n",
      "Epoch 4: Loss = 0.5893, Acc = 79.56%\n",
      "Epoch 5: Loss = 0.4909, Acc = 83.03%\n",
      "Epoch 6: Loss = 0.4068, Acc = 85.90%\n",
      "Epoch 7: Loss = 0.3273, Acc = 88.53%\n",
      "Epoch 8: Loss = 0.2648, Acc = 90.70%\n",
      "Epoch 9: Loss = 0.2191, Acc = 92.24%\n",
      "Epoch 10: Loss = 0.1824, Acc = 93.60%\n",
      "Epoch 11: Loss = 0.1611, Acc = 94.29%\n",
      "Epoch 12: Loss = 0.1435, Acc = 95.07%\n",
      "Epoch 13: Loss = 0.1258, Acc = 95.74%\n",
      "Epoch 14: Loss = 0.1153, Acc = 96.02%\n",
      "Epoch 15: Loss = 0.1086, Acc = 96.23%\n",
      "Epoch 16: Loss = 0.1000, Acc = 96.54%\n",
      "Epoch 17: Loss = 0.0977, Acc = 96.65%\n",
      "Epoch 18: Loss = 0.0881, Acc = 96.93%\n",
      "Epoch 19: Loss = 0.0860, Acc = 97.14%\n",
      "Epoch 20: Loss = 0.0816, Acc = 97.19%\n",
      "Epoch 21: Loss = 0.0805, Acc = 97.24%\n",
      "Epoch 22: Loss = 0.0745, Acc = 97.54%\n",
      "Epoch 23: Loss = 0.0745, Acc = 97.51%\n",
      "Epoch 24: Loss = 0.0749, Acc = 97.46%\n",
      "Epoch 25: Loss = 0.0627, Acc = 97.92%\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(\"Training language model on WikiText-2 (25 epoch, sanity check)...\")\n",
    "train_language_model(model_lm, wikitext2_loader, device, epochs=25)\n",
    "\n",
    "print(\"Training ResNet-18 on CIFAR-10 (25 epoch, sanity check)...\")\n",
    "train_cifar_model(model_cifar, cifar10_loader, device, epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab32f9b-50d4-403b-808b-e7fbd24b0403",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2beda76-0bd5-4d81-b0ac-60a3636d4aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models saved after 25 epochs.\n"
     ]
    }
   ],
   "source": [
    "# Save models\n",
    "torch.save(model_lm.state_dict(), os.path.join(processed_dir, \"simple_transformer_lm.pt\"))\n",
    "torch.save(model_cifar.state_dict(), os.path.join(processed_dir, \"resnet18_cifar10.pt\"))\n",
    "print(\"Models saved after 25 epochs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29d1d2f-d75a-40fe-9fc7-e0afa399303b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5634410-c4b4-4df8-b503-a646c3e8331f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
